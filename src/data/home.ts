// Home page featured projects data

export interface HomeProject {
  id: string;
  title: string;
  subtitle: string;
  description: string;
  image: string;
  videoUrl?: string;
  learnMoreUrl?: string;
  githubUrl?: string;
  features: string[];
  stats?: { label: string; value: string }[];
}
const imgPath = import.meta.env.BASE_URL 
export const homeProjects: HomeProject[] = [
  {
    id: 'hero',
    title: 'AgiBot Research',
    subtitle: 'Pioneering Embodied Intelligence',
    description: '', //
    image: `${imgPath}images/futuristic_ai_brain_neural_network_dark_hero.jpg`,
    learnMoreUrl: '/about',
    features: [
      'Vision Language Action Model',
      'Reinforcement Learning',
      'Embodied World Model',
      //'Physical Intelligence Research'
    ]
  },
  {
    id: 'genie-envisioner',
    title: 'Genie Envisioner',
    subtitle: 'Unified World Foundation Platform',
    description: 'Genie Envisioner is a unified world foundation platform that integrates manipulation policy learning and evaluation within a single video-generative framework.',
    image: `${imgPath}images/virtual_world_model_simulation_ai_genie_envisioner.jpg`,
    learnMoreUrl: 'https://genie-envisioner.github.io/',
    githubUrl: 'https://github.com/AgibotTech/Genie-Envisioner',
    features: [
      'Dual-arm robotic manipulation',
      'High-fidelity closed-loop simulation',
      'Long-term historical context',
      'Multi-view video generation'
    ],
    stats: [
      { label: 'Arms', value: 'Dual' },
      { label: 'Views', value: 'Multi' },
      { label: 'Episodes', value: '1M' },
      { label: '', value: '' }
    ]
  },
  {
    id: 'go-1-model',
    title: 'Genie Operator-1 (GO-1) Model',
    subtitle: 'An Evolution from VLA to ViLLA',
    description: 'Compared to the Vision-Language-Action (VLA) model, where actions are directly conditioned on vision and language inputs, the ViLLA model predicts latent action tokens, bridging the gap between image-text inputs and robot actions generated by the action expert.',
    image: `${imgPath}images/transformer_architecture_large_language_model_ai_diagram.jpg`,
    learnMoreUrl: 'https://agibot-world.com/blog/go1',
    githubUrl: 'https://github.com/OpenDriveLab/AgiBot-World',
    features: [
      '2B parameter unified multimodal architecture',
      'Real-time sensorimotor integration',
      'Natural language instruction following',
      // 'Adaptive behavior learning'
    ],
    stats: [
      { label: 'Parameters', value: '2B' },
      { label: 'Modalities', value: '3' },
      // { label: 'Tasks', value: '100+' },
      { label: 'Success Rate', value: '78%' }
    ]
  },
  {
    id: 'agibot-world',
    title: 'AgiBot World',
    subtitle: 'Comprehensive Embodied AI Dataset',
    description: 'A revolutionary multi-modal dataset capturing diverse real-world interactions for training embodied AI agents. Featuring over 1M episodes of robot-environment interactions across 1000+ scenarios.',
    image: `${imgPath}images/ai_neural_network_data_visualization_concept.jpg`,
    githubUrl: 'https://github.com/OpenDriveLab/AgiBot-World',
    learnMoreUrl: 'https://agibot-world.com',
    features: [
      'Multi-modal sensory data (RGB, Depth, Tactile)',
      '1,000,000+ Trajectorys',  
      // 'Standardized evaluation protocols',
      // 'Continuous expansion with community contributions'
    ],
    stats: [
      { label: 'Trajs', value: '1M' },
      { label: 'Tasks', value: '217' },
      { label: 'Hours', value: '2976' },
      { label: 'Skills', value: '87' },
      { label: 'Scenes', value: '106' }
    ]
  },  
];
